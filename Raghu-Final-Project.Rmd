---
title: "Assingment"
author: "Raghu Annavarapu"
Date: "April 16, 2020"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## The Impact of Inflation on Economic Growth - United States
The objetcive of this study is to investigate inflation rate and its impact on the GDP growth rate for United States, using IMF data for the period 1981-2024, on an annual basis. For conducting the study, and achieving results, a multiple linear regression model with the least squares regression is used. Moreover, multiple linear regression analysis has been applied in order to investigate whether Inflation rate, as an independent variable, has any significant
impact on economic growth. Also, considered economic indicators such as Gross national savings, Total investment, Volume of imports of goods and services, Volume of exports of goods and services, Unemployment rate, Current account balance in percentages format for further analysis.

```{r}
# read csv files
library(readr)
library(psych)
library(tidyverse)
library(Hmisc)
library(car)
# read csv files
my_data <- read.csv("C:\\Users\\Dell\\Desktop\\Raghu\\data2.csv",header = TRUE)
#Summary of Data
head(my_data)
dim(my_data)
str(my_data)
names(my_data)
#check for nulls in the data
NA_values=data.frame(no_of_na_values=colSums(is.na(my_data)))
head(NA_values,9)
```

Observations: There are 9 varibales for 44 years which includes forecasted data upto 2024 from 1981. The data is collected from IMF website and there are no missing values to perform imputation. Also, the data is collected in the percentages, so no scaling or normalization is done on the data. In the data set, year varibale is not required for analysis.

Data cleansing and exploratory analysis:
```{r}
#Removing year variable
my_data <- subset(my_data, select = -c(1))
head(my_data)
summary(my_data)
```
Multiple Linear Regression:
Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. Every value of the independent variable x is associated with a value of the dependent variable y. This linear equation describes how the mean response y changes with the explanatory variables. The observed values for y vary about their means y and are assumed to have the same standard deviation.

Multiple Linear Regression Important Points:
1. There must be linear relationship between independent and dependent variables
2. Multiple regression suffers from multicollinearity, autocorrelation, heteroskedasticity.
3. Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.
4. Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is that the coefficient estimates are unstable.
5. In case of multiple independent variables, we can go with forward selection, backward elimination and step wise approach for selection of most significant independent variables.

Model 1 to predict results:
```{r}

#Regression Model
model_1 = lm(GDP_GROWTH~., my_data)
summary(model_1)
```
As in our model the adjusted R-squared: 0.8401, meaning that independent variables explain 84% of the variance of the dependent variable, only 4 variables are significant out of 8 independent variables.

The p-value of the F-statistic is less than 0.05(level of Significance), which means our model is significant. This means that, at least, one of the predictor variables is significantly related to the outcome variable.

Our model equation can be written as:
GDP_GROWTH = -6.65689 + 0.415*TI_P_GDP +0.014*GNS_P_GDP - 0.223*INFLATION_GROWTH + 0.186*VOLUME_IGS_PC +0.054*VOLUME_EGS_PC - 0.048*UNEMPLOYEMENT_P_LF + 0.151*CUR_BAL_P_GDP

```{r}
# Stepwise Regression
library(MASS)
fit <- lm(GDP_GROWTH~., my_data)
step <- stepAIC(fit, direction="both")
step$anova # display results
```
Final Model from results after eliminatig 3 insigificat variables:GDP_GROWTH ~ TI_P_GDP + INFLATION_GROWTH + VOLUME_IGS_PC + VOLUME_EGS_PC + CUR_BAL_P_GDP.

```{r}
#Regression Model
model_2 = lm(GDP_GROWTH~TI_P_GDP + INFLATION_GROWTH + VOLUME_IGS_PC + VOLUME_EGS_PC + CUR_BAL_P_GDP, my_data)
require(fmsb)
VIF(model_2)
```

Multicollinearity Test using Variable Inflation Factor (VIF): High Variable Inflation Factor (VIF) is a sign of multicollinearity. There is no formal VIF value for determining the presence of multicollinearity; however, in weaker models, VIF value greater than 10 may be a cause of major concern. VIF is less than 10, multicolinearity is not suggested.

```{r}

lmtest::bptest(model_2)
car::ncvTest(model_2)
```
Heteroscedasticity Test:
For this purpose, there are a couple of tests that comes handy to establish the presence or absence of heteroscedasticity – The Breush-Pagan test and the NCV test.

Both these test have a p-value less that a significance level of 0.05, therefore we can reject the null hypothesis that the variance of the residuals is constant and infer that heteroscedasticity is indeed present.

Model 2 to predict final results:
```{r}

#Regression Model
model_2 = lm(GDP_GROWTH~TI_P_GDP + INFLATION_GROWTH + VOLUME_IGS_PC + VOLUME_EGS_PC + CUR_BAL_P_GDP, my_data)
summary(model_2)
```
As in our model the adjusted R-squared: 0.8463, meaning that independent variables explain 84% of the variance of the dependent variable, only 4 variables are significant out of 5 independent variables.

The p-value of the F-statistic is less than 0.05(level of Significance), which means our model is significant. This means that, at least, one of the predictor variables is significantly related to the outcome variable.

Our final model equation can be written as:
GDP_GROWTH = -7.654 + 0.4669*TI_P_GDP - 0.253*INFLATION_GROWTH + 0.1792*VOLUME_IGS_PC +0.0595*VOLUME_EGS_PC + 0.16459*CUR_BAL_P_GDP

Model3 & Results:
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```
## Exploratory Factor Analysis
Exploratory factor analysis is an interdependence technique whose primary purpose is to define the underlying structure among the variables in the analysis.

Exploratory Factor Analysis: 

1. Examines the interrelationships among a large   number of variables and then attempts to explain them in terms of their common underlying dimensions.
2. These common underlying dimensions are referred to as factors.
3. A summarization and data reduction technique that does not have independent and dependent variables, but is an interdependence technique in which all variables are considered simultaneously.

```{r}
my_data <- read.csv("C:\\Users\\Dell\\Desktop\\Raghu\\data3.csv",header = TRUE)
#check for nulls in the data
NA_values=data.frame(no_of_na_values=colSums(is.na(my_data)))
head(NA_values,11)
```

```{r}
#Removing year variable
my_data <- subset(my_data, select = -c(1))
dim(my_data)
str(my_data)
names(my_data)
summary(my_data)
```

## Correlation Matrix
```{r}
#Correlation Matrix
cor_m=cor(my_data)
library(corrplot)
corrplot(cor_m, type = "upper", order = "hclust", tl.col = "black", tl.srt =45)
```

We can see highly correlated variables using inter-item correlation analysis:

1. Volume Imports Goods Services Percentage Change is higly correlated with GDP Growth
2. Volume Exports Goods Services Percentage Change is higly correlated with GDP Growth
3. Volume Exports Goods Services Percentage Change is higly correlated with Volume Imports Goods Services Percentage Change
4. Government Expenditure Percentage of GDP is highly correlated with Unemployment percentage of Labor Force
5. Government Expenditure Percentage of GDP is highly correlated with Gross National Savings Percentage of GDP
6. Total Investment Percentage of GDP is highly correlated with Government Expenditure Percent, Government Revenue Percent, Unemployment percentage of Labor Force.
```{r}
cor_m2 <- rcorr(as.matrix(my_data), type="pearson")
# Extract the correlation coefficients
cor_m2$r
# Extract p-values
cor_m2$P
# Insignificant correlations are leaved blank
corrplot(cor_m2$r, type="upper", order="hclust",
         p.mat = cor_m2$P, sig.level = 0.01, insig = "blank")
```

Computed p value for correlation pairs and plotted only significance pairs.

rcorr-Computes a matrix of Pearson's-r or Spearman's rho rank correlation coefficients for all possible pairs of columns of a matrix. Missing values are deleted in pairs rather than deleting all rows of x having any missing variables.

## Testing for FA - Kaiser-Meyer-Olkin (KMO) 

1. Test measures the suitability of data for factor analysis
2. KMO values range between 0 and 1
3. Value of KMO less than 0.5 is considered inadequate
```{r}
data_fa <- my_data[,-10]
datamatrix <- cor(data_fa)
KMO(r=datamatrix)
```

Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variable. KMO values range between 0 and 1. Value of KMO less than 0.5 is considered inadequate. 

We need to remove the response variable. The test value is 0.53 and factor analysis is appropriate.

## Number of Factors

1. Calculate eigen values
2. Plot eigen values in a scree plot
3. Determine Number of factors

```{r}
library(nFactors)
ev <- eigen(cor(data_fa))
ev$values
ap <- parallel(subject=nrow(data_fa),var=ncol(data_fa),
  rep=100,cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
```

A crucial decision in EFA is how many factors to extract. nFactors is a package which offers a suite of functions to help in this decision. One way to determine the number of factors or components in a correlation matrix is to examine the “scree” plot of the successive eigen values. Based on the scree plot, we can decide 4 optimal factors.

## Run Analysis

```{r}
nfactors <- 4
fit1 <-factanal(data_fa,nfactors,scores = c("regression"),rotation = "varimax")
print(fit1)

fa_var <-  fa(r=data_fa, nfactors = 4, rotate='varimax',fm="pa")
fa.diagram(fa_var)

```

The output gives us the summary for loadings (weights of variable for each factor), the cumulative proportion of factors, which is 69% of explaining data variance.

Factor analysis results are typically interpreted in terms of the major loadings on each factor. These structures may be represented as a table of loadings or graphically, where all loadings with an absolute value > some cut point are represented as an edge (path).

Factor 1 accounts for 33.4% of the variance; Factor 2 accounts for 19.50% of the variance; Factor 3 accounts for 18.90% of the variance; Factor 4 accounts for 11.50% of the variance. All the 4 factors together explain for 83% of the variance in performance.

We can effectively reduce dimensionality from 10 to 4 while only losing about 17% of the variance.

## Regression

1. Extract scores from factor analysis
2. Combine response and predictors
3. Label factors

```{r}
fa_var <-  fa(r=data_fa, nfactors = 4, rotate='varimax',fm='pa')
#fa.diagram(fanone)

head(fa_var$scores)
regdata <- cbind(my_data[10], fa_var$scores)
#Labeling the data
names(regdata) <- c("GDP_GROWTH", "TI_GOVT_R_E", "CUR_BAL_GNS","GOODS_SERVICES", "INFL_UNEMPL")


set.seed(100)
indices= sample(1:nrow(regdata), 0.7*nrow(regdata))
train=regdata[indices,]
test = regdata[-indices,]

#Regression Model using train data
model1 = lm(GDP_GROWTH~., train)
summary(model1)
library(Metrics)
pred_test1 <- predict(model1, newdata = test, type = "response")

test$GDP_GROWTH_Predicted <- pred_test1
head(test[c(1,6)], 5)


#model with out factors
model <- lm(GDP_GROWTH ~., data = my_data)
summary(model)

```
The factors CUR_BAL_GNS, GOODS_SERVICES, INFL_UNEMPL are highly significant and TI_GOVT_R_E is not significant in the model.

The model performance is good in predicting results with Adjusted R Squared of 80.9%. We have also given the results of the model built with out factor analysis. We can see the significant improvement in the Adjusted R Squared from 72.5% to 80.9%.

Findings:

As per the results method 2 is more significant. The generated econometric results show that Inflation Rate variable is statistically significant with a confidence level of 99.9 per cent with a negative impact (P = .007). This means that any increase in the Inflation Rate has a negative impact on economic growth rate. The generated results shows that the total investment, Volume of imports of goods and services variables are statistically significant with a confidence level of 99.9 per cent with a positive impact.

Thank You,
Raghu